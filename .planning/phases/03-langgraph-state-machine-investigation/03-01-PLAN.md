---
phase: 03-langgraph-state-machine-investigation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - veritas/tests/langgraph_investigation/test_01_minimal_graph.py
autonomous: true
requirements:
  - CORE-03
  - CORE-03-2
  - CORE-03-3

must_haves:
  truths:
    - Minimal StateGraph with async node executes via ainvoke() without CancelledError on Python 3.11+
    - Graph.get_graph().print_ascii() produces visualization output
    - Error logging captures any exceptions or CancelledError if they occur
    - Test can be run directly and via pytest
  artifacts:
    - path: veritas/tests/langgraph_investigation/test_01_minimal_graph.py
      provides: Isolated minimal reproduction test for LangGraph ainvoke()
      min_lines: 80
      contains: test_ainvoke_minimal_graph, test_ainvoke_vs_manual, test_graph_visualization
    - path: veritas/tests/langgraph_investigation/README.md
      provides: Investigation documentation entry point
      min_lines: 30
      contains: Investigation approach, test descriptions, documentation of findings
  key_links:
    - from: test_01_minimal_graph.py
      to: langgraph.graph.StateGraph
      via: StateGraph import and compile()
      pattern: from langgraph.graph import StateGraph.*\.compile\(\)
    - from: test_01_minimal_graph.py
      to: asyncio
      via: await graph.ainvoke()
      pattern: await (graph|compiled)\.ainvoke\(

---

<objective>
Create isolated minimal reproduction test to verify LangGraph ainvoke() behavior with Python 3.11+ asyncio, stripping away VERITAS complexity to isolate CancelledError root cause.

Purpose: Establish baseline LangGraph behavior on minimal graph, observe any async issues, and gather first-hand evidence of whether CancelledError actually occurs on current Python version (3.11.5).

Output: `veritas/tests/langgraph_investigation/test_01_minimal_graph.py` with minimal graph tests, `veritas/tests/langgraph_investigation/README.md` documentation.
</objective>

<execution_context>
@C:/Users/hp/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/hp/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-langgraph-state-machine-investigation/03-CONTEXT.md
@.planning/phases/03-langgraph-state-machine-investigation/03-RESEARCH.md
@.planning/STATE.md
@.planning/ROADMAP.md
@C:/files/coding dev era/elliot/elliotAI/.planning/research/LANGGRAPH.md
@C:/files/coding dev era/elliot/elliotAI/.planning/codebase/TESTING.md

# Codebase references
@C:/files/coding dev era/elliot/elliotAI/veritas/core/orchestrator.py
@C:/files/coding dev era/elliot/elliotAI/veritas/tests/test_veritas.py
</context>

<tasks>

<task type="auto">
  <name>Create investigation directory and minimal graph tests</name>
  <files>veritas/tests/langgraph_investigation/test_01_minimal_graph.py</files>
  <action>
    Create directory `veritas/tests/langgraph_investigation/` and file `test_01_minimal_graph.py` with minimal reproduction test:

    1. Imports: `asyncio`, `pytest`, `typing.TypedDict`, `langgraph.graph.StateGraph, END`, `sys`, `time`, `pathlib.Path`

    2. Add sys.path to import veritas modules: `sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))`

    3. Define `MinimalState(TypedDict)` with fields:
       - `count: int`
       - `status: str`
       - `error_log: list[str]`

    4. Create `async_node(state: MinimalState) -> dict` function that:
       - Awaits `asyncio.sleep(0.01)` to simulate async work
       - Returns `{"count": state.get("count", 0) + 1}`
       - Wraps in try/except to log errors to state["error_log"]

    5. Create `build_minimal_graph()` function that:
       - Builds StateGraph with MinimalState
       - Adds 2-3 nodes using async_node
       - Creates simple routing (sequential or conditional)
       - Returns compiled graph: `return graph.compile()`

    6. `test_ainvoke_minimal_graph()` test (marked @pytest.mark.asyncio):
       - Build graph via build_minimal_graph()
       - Create initial state
       - Execute via `await graph.ainvoke(initial_state)`
       - Assert result matches expectations
       - Catch any `asyncio.CancelledError` and fail with descriptive message
       - Log Python version: `sys.version_info`

    7. `test_ainvoke_vs_manual_execution()` test:
       - Run same test via ainvoke()
       - Run manual sequential execution (await each node, update state)
       - Compare results are identical
       - This proves ainvoke behavior matches manual execution

    8. `test_graph_visualization()` test:
       - Build graph
       - Call `graph.get_graph().print_ascii()` to generate visualization
       - Capture or print output
       - Verify graph structure is correct

    DO NOT use any VERITAS agents or NIMClient - this is pure LangGraph isolation test.
  </action>
  <verify>
    cd veritas && python -m pytest tests/langgraph_investigation/test_01_minimal_graph.py -v
  </verify>
  <done>
    Minimal graph test file created with 3 async tests, all passing, documenting Python 3.11.5 LangGraph ainvoke() behavior.
  </done>
</task>

<task type="auto">
  <name>Create investigation README entry point</name>
  <files>veritas/tests/langgraph_investigation/README.md</files>
  <action>
    Create `veritas/tests/langgraph_investigation/README.md` with:

    1. Phase 3 investigation overview (purpose, Python version, test approach)

    2. Test directory structure:
       - test_01_minimal_graph.py - Minimal isolated reproduction
       - test_02_full_audit_mocked.py - Full audit with mocked NIMClient
       - test_03_behavioral_differences.py - Execution flow comparison

    3. Investigated components section:
       - LangGraph internals (StateGraph lifecycle, async event handling)
       - NIMClient interaction (async operations with state machine)
       - Subprocess orchestrator (Windows + subprocess isolation)

    4. Running tests section with pytest commands

    5. Expected findings section (template for documenting results)

    Use markdown format, include Python version note (actual running: 3.11.5, not 3.14 as documented).
  </action>
  <verify>
    cat veritas/tests/langgraph_investigation/README.md
  </verify>
  <done>
    Investigation README created with test descriptions, purpose, and documentation framework for findings.
  </done>
</task>

</tasks>

<verification>
1. test_01_minimal_graph.py imports and runs without errors
2. All 3 tests pass (ainvoke, manual comparison, visualization)
3. Python version is logged in test output (3.11.5)
4. README.md exists with investigation documentation
5. LangGraph ainvoke() behavior on minimal graph is documented
</verification>

<success_criteria>
1. Minimal reproduction test executes successfully
2. No CancelledError observed on Python 3.11.5 (or error is captured with full traceback)
3. Graph visualization produces output
4. Manual execution matches ainvoke execution
5. Investigation documentation framework created
</success_criteria>

<output>
After completion, create `.planning/phases/03-langgraph-state-machine-investigation/03-01-SUMMARY.md`
</output>
