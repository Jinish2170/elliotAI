---
phase: 03-langgraph-state-machine-investigation
plan: 02
type: execute
wave: 2
depends_on:
  - 03-01
files_modified:
  - veritas/tests/langgraph_investigation/test_02_full_audit_mocked.py
autonomous: true
requirements:
  - CORE-03
  - CORE-03-2
  - CORE-03-5

must_haves:
  truths:
    - Full audit with mocked NIMClient executes via ainvoke() on actual StateGraph
    - All 5 agent nodes (Scout, Security, Vision, Graph, Judge) run through properly
    - Sequential execution fallback maintains compatibility
    - Event flow between nodes is observable and logged
  artifacts:
    - path: veritas/tests/langgraph_investigation/test_02_full_audit_mocked.py
      provides: Full audit test with mocked external dependencies
      min_lines: 120
      contains: test_ainvoke_full_audit_mocked, test_sequential_execution_fallback
    - path: veritas/tests/langgraph_investigation/conftest.py
      provides: Shared fixtures for investigation tests
      min_lines: 40
      contains: mock_nim_client, mock_scout, audit_state_fixture
  key_links:
    - from: test_02_full_audit_mocked.py
      to: veritas.core.orchestrator.build_audit_graph
      via: Import and graph construction
      pattern: from core.orchestrator import build_audit_graph.*build_audit_graph\(\)
    - from: test_02_full_audit_mocked.py
      to: unittest.mock.patch
      via: Mock decorator for agents and NIMClient
      pattern: @patch\(.*[Ss]cout.*\)|@patch\(.*[Nn][Ii][Mm].*\)
    - from: conftest.py
      to: test_02_full_audit_mocked.py
      via: pytest fixture loading
      pattern: def test_.*\(mock_nim|fixture

---

<objective>
Create full audit test with mocked NIMClient and agents to observe real execution path of VERITAS StateGraph via ainvoke(), comparing with sequential execution fallback.

Purpose: Test actual VERITAS orchestrator build_audit_graph() with ainvoke() using mocked external dependencies to isolate LangGraph execution from NIM API calls and browser automation.

Output: `veritas/tests/langgraph_investigation/test_02_full_audit_mocked.py` with full audit test, `conftest.py` with shared fixtures.
</objective>

<execution_context>
@C:/Users/hp/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/hp/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-langgraph-state-machine-investigation/03-CONTEXT.md
@.planning/phases/03-langgraph-state-machine-investigation/03-RESEARCH.md
@.planning/STATE.md
@.planning/ROADMAP.md

# Codebase references
@C:/files/coding dev era/elliot\elliotAI/veritas/core/orchestrator.py (build_audit_graph, audit() method)
@C:/files/coding dev era\elliot\elliotAI/veritas/core/nim_client.py
@C:/files/coding dev era\elliot\elliotAI/veritas/agents/scout.py
@C:/files/coding dev era\elliot\elliotAI/veritas/tests/test_veritas.py

# Prior plan
@.planning/phases/03-langgraph-state-machine-investigation/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Create conftest.py with investigation fixtures</name>
  <files>veritas/tests/langgraph_investigation/conftest.py</files>
  <action>
    Create `veritas/tests/langgraph_investigation/conftest.py` with shared investigation test fixtures:

    1. Imports: `pytest`, `unittest.mock.AsyncMock, MagicMock`, `dataclasses`, `sys`, `pathlib.Path`

    2. Add sys.path for veritas imports: `sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))`

    3. `@pytest.fixture` `mock_nim_client()` that:
       - Creates MagicMock instance
       - Mocks `analyze_image` as AsyncMock returning cached VLM response
       - Mocks `generate_text` as AsyncMock returning cached text response
       - Sets `call_count = 0` for tracking
       - Returns instance

    4. `@pytest.fixture` `mock_scout()` that:
       - Creates AsyncMock for Scout class
       - Mocks `__aenter__` and `__aexit__`
       - Mocks `investigate` as AsyncMock returning ScoutResult-like dict
       - Returns mocked instance

    5. `@pytest.fixture` `audit_state()` that:
       - Returns minimal valid AuditState TypedDict
       - Includes required fields: url, audit_tier, iteration, max_iterations, max_pages, status, scout_results, vision_result, graph_result, judge_decision, pending_urls, investigated_urls, start_time, elapsed_seconds, errors, scout_failures, nim_calls_used, site_type, site_type_confidence, verdict_mode, security_results, security_mode, enabled_security_modules

    Follow existing veritas test patterns for mocking and fixture structure.
  </action>
  <verify>
    python -c "from veritas.tests.langgraph_investigation.conftest import *; print('OK')"
  </verify>
  <done>
    conftest.py created with mock_nim_client, mock_scout, audit_state fixtures importable without errors.
  </done>
</task>

<task type="auto">
  <name>Create full audit test with mocked NIMClient</name>
  <files>veritas/tests/langgraph_investigation/test_02_full_audit_mocked.py</files>
  <action>
    Create `veritas/tests/langgraph_investigation/test_02_full_audit_mocked.py` with full audit test:

    1. Imports: `pytest`, `asyncio`, `unittest.mock.patch, AsyncMock, MagicMock`, `dataclasses`, `sys`, `pathlib.Path`, `time`

    2. Add sys.path and import veritas modules:
       - `from core.orchestrator import build_audit_graph, AuditState`

    3. `@pytest.mark.asyncio` test `test_ainvoke_full_audit_mocked(mock_nim_client, mock_scout, audit_state)`:
       - Patch `veritas.agents.scout.StealthScout` to return mock_scout
       - Patch `veritas.core.nim_client.NIMClient` to return mock_nim_client
       - Patch `veritas.agents.vision.VisionAgent` with mocked analyze
       - Build graph via `build_audit_graph()`
       - Compile graph
       - Execute via `await compiled.ainvoke(audit_state)`
       - Assert result status is "completed" or "error"
       - Catch any `asyncio.CancelledError` with descriptive error
       - Document which nodes executed successfully

    4. `@pytest.mark.asyncio` test `test_sequential_execution_fallback(mock_nim_client, mock_scout, audit_state)`:
       - Same patches as above
       - Import `VeritasOrchestrator` from core.orchestrator
       - Create orchestrator instance from audit_state
       - Run `await orchestrator.audit()` to test sequential fallback
       - Assert audit completes without errors
       - Verify result matches ainvoke execution (if both work)

    5. `@pytest.mark.asyncio` test `test_graph_structure_and_nodes(mock_nim_client, mock_scout)`:
       - Build graph and compile
       - Call `graph.get_graph().print_ascii()` to visualize structure
       - Verify all 5 nodes present: scout_node, security_node, vision_node, graph_node, judge_node
       - Verify routing edges exist

    Use patches correctly: `@patch('module.path.to.Class', return_value=mock_instance)`
    DO NOT make real NIM calls or use real browser (Scout) - all external deps mocked.
  </action>
  <verify>
    cd veritas && python -m pytest tests/langgraph_investigation/test_02_full_audit_mocked.py -v --tb=short
  </verify>
  <done>
    Full audit test created with 3 tests, all external dependencies mocked, ainvoke() execution tested against sequential fallback.
  </done>
</task>

</tasks>

<verification>
1. conftest.py fixtures import cleanly
2. Full audit test patches all external dependencies correctly
3. ainvoke() test completes (passes or fails with CancelledError traceback)
4. Sequential fallback test passes
5. Graph structure visualization test runs
6. NO real NIM API calls or browser automation in test
</verification>

<success_criteria>
1. Full audit with mocked dependencies executes via ainvoke()
2. Sequential execution fallback verified to maintain compatibility
3. Graph structure and all 5 nodes verified present
4. Comparison between ainvoke() and sequential execution documented
5. Any CancelledError captured with full traceback for analysis
</success_criteria>

<output>
After completion, create `.planning/phases/03-langgraph-state-machine-investigation/03-02-SUMMARY.md`
</output>
